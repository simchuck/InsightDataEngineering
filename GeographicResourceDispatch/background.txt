

"""
Since the cost to generate electricity is different for each power plant, the cost to generate each kilowatt-hour (KWH) of electricity changes constantly depending on which plants are operating and how cost effective those plants are. If we don't consider transmission congestion and emissions limitations for the moment, we should consider that there are many generators producing the necessary electricity at any given time. And, they are generally operated on an "economic dispatch"basis
"""


Market forces and particular characteristics of any specific resource node are unique relative to that of other nodes, and therefore the "price" for each node will be different.  There are several external factors that vary with time, and therefore cause a variation in the cost of utilizing the resource.  If we can distribute our workflow at will across the available resource nodes, we can potentially optimize the performance of the system based on our specific requirements.  Assuming that we want to minimize the total operaional cost...



The vision for my project imagines an available ecosystem of geographically distributed "resource nodes" which provide computing services (e.g., server farms) and offer these services under time-of-use pricing.  Each resource node has an available capacity to do work at any given time.  We are provided with two real-time API streams (?) from the resource node:
    
    <node>:resource_price
    <node>:resource_capacity

We will also know the geographic location of the node, and perhaps some statistics related to latency, availability, throughput rates, etc.  These factors could become important for some optimization strategies, depending on the particular needs.  

Since this vision is not an accurate representation of present-day realities, I need to implement various stream processing tasks to simulate the above API streams.  The resource price depends on the business model for each resource node, and will include considerations for operating costs that can be related directly to the service provided, as well as overhead costs and profit margins.  We generally don't need to know any of this, although there are potential enhanced optimizations that would be possible from some of the intermediate parameters.  

Our simulation stream will include the following:

    <node>:resource_weather         - used to predict energy demand associated with cooling systems at the resource
    <node>:resource_server_attrs    - simulated attributes of the server instances during operation, used to model energy demand for the servers
    <node>:resource_cooling_energy  - 
    <node>:resource_energy_demand   - includes server energy and cooling energy, and possibly additional energy consumption
    <node>:resource_energy_price    - used to determine the operating costs associated with energy used by the resource
    <node>:resource_price           - unit pricing for the resources services, includes cost of energy, overhead, profits, etc.  
    <node>:resource_capacity        - possibly derived from the server attributes, representing the available service capacity for the resource

In an ideal situation, we might have real-time data streams for the energy price (from real-time energy market), energy demand (from facility metering), and weather (from weather stations), in addition to the price and capacity streams for the service (provided as part of the service agreement for the resource).  These additional streams would allow for some predictive capabilities to further enhance the optimizations.  It should also be noted that forecasted streams are available for both the energy price (from ISO/RTO real-time energy markets) and weather (from publicly available weather stations, via NOAA or others).  

In order to provide a mechanism for evaluating the robustness of the system, as well as providing data streams of the desired characteristics, I will also have simulation tools for the following tasks:

    interpolative fit algorithm to divide an existing static dataset into arbitrarily small time intervals.  This should include capability to vary the fitting algorithm (which uses some subset of data points around the time period to be interpolated), as well as random distribution functions to simulate uncertainty in the simulated value relative to the fitting algorithm.  
    networking algorithm to simulate latency and failures in the timing of the data stream records.  This might be driven by random distribution functions for both the uncertainty around timing (relative to a regular time interval) and probability of communication or node failures resulting in dropped/missed records.  

Once we have the above algorithms available, we can apply them to an arbitrary number of resource nodes, each with its particular attributes as identified above.  


All of the above is really just about generating the input data streams for our application.  The meat of the project is concerned with using this information to dispatch processing jobs across the available resource nodes according to some optimization algorithm.  The grand vision would be to process a prioritized list of jobs to generate a work package queue that partitions/parses each job in to smaller pieces and identifies the optimum dispatch instructions for sending these pieces to specific resource nodes across the network of available resources.  

There be dragons here.  

That vision includes (at least) one data engineering problem within another.  For my project, I am focusing on the "exterior" problem of determining the best dispatch scenario in real-time.  I will avoid the problem of partitioning jobs, and assume that the jobs to be processed have been reduced to their most atomic state - the smallest practical job size.  It seems that the optimization (for lowest cost) becomes relatively trivial at any point in time - if the highest priority job can fit within the available capacity of the lowest price resource then it will be dispatched to that resource.  We continue down through our prioritized work queue until we consume all of the available capacity in our nodes (subject to any other constraints we might impose).  Then we repeat this process for each processing interval.  The project becomes most interesting when we can allow for arbitrarily fast data streams, and arbitrarily large networks (number of resource nodes).  

We will ignore for now inefficiencies related to moving a task from one node to another.  The optimization for cost would likely sacrifice processing speed as a secondary concern, particularly since the transfer of data from one node to another across the network is expected to be significantly time-expensive.  Alternately, we can (for now) impose the additional constraint that once dispatched, the job is allowed to run to completion.  

Note that I am not concerning myself with how to split up a job, or to recombine the results of that job (map-reduce?) in this project.  
